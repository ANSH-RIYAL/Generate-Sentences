{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import itertools\n",
    "import nltk\n",
    "import os\n",
    "import operator\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\anshr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Do this only the first time\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "corpora_dir = \"C:/Users/anshr/AppData/Roaming/nltk_data/corpora/state_union\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data!!! OMG SOOO OFFICIALLL!! AAHHH\n",
      "\n",
      "\n",
      "\n",
      "[\"PRESIDENT HARRY S. TRUMAN'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS April 16, 1945Mr.\", 'Speaker, Mr. President, Members of the Congress:It is with a heavy heart that I stand before you, my friends and colleagues, in the Congress of the United States.Only yesterday, we laid to rest the mortal remains of our beloved President, Franklin Delano Roosevelt.', 'At a time like this, words are inadequate.', 'The most eloquent tribute would be a reverent silence.Yet, in this decisive hour, when world events are moving so rapidly, our silence might be misunderstood and might give comfort to our enemies.In His infinite wisdom, Almighty God has seen fit to take from us a great man who loved, and was beloved by, all humanity.No man could possibly fill the tremendous void left by the passing of that noble soul.', 'No words can ease the aching hearts of untold millions of every race, creed and color.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading Data!!! OMG SOOO OFFICIALLL!! AAHHH\\n\\n\\n\")\n",
    "file_list = []\n",
    "for root,_,files in os.walk(corpora_dir):\n",
    "    for filename in files:\n",
    "        file_list.append(os.path.join(root,filename))\n",
    "\n",
    "sentences = []\n",
    "for files in file_list:\n",
    "    with open(files,'r') as fin:\n",
    "        try:\n",
    "            str_form = fin.read().replace('\\n','')\n",
    "            sentences.extend(nltk.sent_tokenize(str_form))\n",
    "        except:\n",
    "            pass\n",
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  19003  unique words...\n"
     ]
    }
   ],
   "source": [
    "# Sentence delimiters\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Word fequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found \", len(word_freq.items()), \" unique words...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13667\n",
      "[['PRESIDENT', 'HARRY', 'S.', 'TRUMAN', \"'S\", 'ADDRESS', 'BEFORE', 'A', 'JOINT', 'SESSION', 'OF', 'THE', 'CONGRESS', 'April', '16', ',', '1945Mr', '.'], ['Speaker', ',', 'Mr.', 'President', ',', 'Members', 'of', 'the', 'Congress', ':', 'It', 'is', 'with', 'a', 'heavy', 'heart', 'that', 'I', 'stand', 'before', 'you', ',', 'my', 'friends', 'and', 'colleagues', ',', 'in', 'the', 'Congress', 'of', 'the', 'United', 'States.Only', 'yesterday', ',', 'we', 'laid', 'to', 'rest', 'the', 'mortal', 'remains', 'of', 'our', 'beloved', 'President', ',', 'Franklin', 'Delano', 'Roosevelt', '.'], ['At', 'a', 'time', 'like', 'this', ',', 'words', 'are', 'inadequate', '.'], ['The', 'most', 'eloquent', 'tribute', 'would', 'be', 'a', 'reverent', 'silence.Yet', ',', 'in', 'this', 'decisive', 'hour', ',', 'when', 'world', 'events', 'are', 'moving', 'so', 'rapidly', ',', 'our', 'silence', 'might', 'be', 'misunderstood', 'and', 'might', 'give', 'comfort', 'to', 'our', 'enemies.In', 'His', 'infinite', 'wisdom', ',', 'Almighty', 'God', 'has', 'seen', 'fit', 'to', 'take', 'from', 'us', 'a', 'great', 'man', 'who', 'loved', ',', 'and', 'was', 'beloved', 'by', ',', 'all', 'humanity.No', 'man', 'could', 'possibly', 'fill', 'the', 'tremendous', 'void', 'left', 'by', 'the', 'passing', 'of', 'that', 'noble', 'soul', '.'], ['No', 'words', 'can', 'ease', 'the', 'aching', 'hearts', 'of', 'untold', 'millions', 'of', 'every', 'race', ',', 'creed', 'and', 'color', '.']]\n",
      "<FreqDist with 19003 samples and 385201 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(tokenized_sentences[:5])\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size =  8000\n",
      "The least used word is \"discriminating\" and it appeared 2 times\n"
     ]
    }
   ],
   "source": [
    "# We limit the total words converted to tokens by giving priority based on frequency\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "print(\"Vocabulary size = \",vocabulary_size)\n",
    "print(\"The least used word is \\\"{}\\\" and it appeared {} times\".format(vocab[-1][0],vocab[-1][1]))\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data\n",
    "# x represents every word except for the last one, y represents every next word.\n",
    "x_train = np.asarray([[word_to_index[w] for w in sentence[:-1]] for sentence in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sentence[1:]] for sentence in tokenized_sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10th input is [722, 3562, 3025, 3563, 822, 1113, 1385, 241, 1114, 1115, 437, 246, 757, 1992, 1720, 1, 7999] and corresponding target output is [3562, 3025, 3563, 822, 1113, 1385, 241, 1114, 1115, 437, 246, 757, 1992, 1720, 1, 7999, 2]\n",
      "In human terms, it looks something like: \n",
      "[('PRESIDENT', 'HARRY'), ('HARRY', 'S.'), ('S.', 'TRUMAN'), ('TRUMAN', \"'S\"), (\"'S\", 'ADDRESS'), ('ADDRESS', 'BEFORE'), ('BEFORE', 'A'), ('A', 'JOINT'), ('JOINT', 'SESSION'), ('SESSION', 'OF'), ('OF', 'THE'), ('THE', 'CONGRESS'), ('CONGRESS', 'April'), ('April', '16'), ('16', ','), (',', 'UNKNOWN_TOKEN'), ('UNKNOWN_TOKEN', '.')]\n"
     ]
    }
   ],
   "source": [
    "x_example, y_example = x_train[0], y_train[0]\n",
    "print(\"The 10th input is {} and corresponding target output is {}\".format(x_example, y_example))\n",
    "print(\"In human terms, it looks something like: \")\n",
    "print(list(zip([index_to_word[x] for x in x_example],[index_to_word[y] for y in y_example])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    t = (np.exp(x)/np.sum(np.exp(x), axis = 0))\n",
    "    print(t.shape)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim = 50, bptt_truncate = 4):\n",
    "        \n",
    "        # Instance variable\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        # Randomly initialize network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, word_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        time_steps = len(x)\n",
    "        # we save all time steps during forward propagation\n",
    "        states = np.zeros((time_steps+1, self.hidden_dim))\n",
    "        # +1 for the first state we have initialised to 0\n",
    "        states[-1] = np.zeros(self.hidden_dim)\n",
    "        # we also save the outputs at each time step\n",
    "        outs = np.zeros((time_steps,self.word_dim))\n",
    "        # print(outs.shape)\n",
    "        print(states.shape)\n",
    "        for t in np.arange(time_steps):\n",
    "            # indexing U by x[t] is the same as multiplying U with One Hot vector\n",
    "            states[t] = np.tanh(self.U[:,x[t]] + self.W.dot(states[t-1]))\n",
    "            s_in = self.V.dot(states[t])\n",
    "            print(s_in.shape)\n",
    "            outs[t] = softmax(s_in)\n",
    "        return [outs,states]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # Perform forward propagation and return index of highest score\n",
    "        outs, states = self.forward_prop(x)\n",
    "        return np.argmax(outs, axis = 1)\n",
    "    \n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "        for i in np.arange(len(y)):\n",
    "            o,s = self.forward_prop(x[i])\n",
    "            correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "            L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    \n",
    "    def calculate_loss(self, x, y):\n",
    "        N = np.sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x,y)/N\n",
    "    \n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        # calculate the gradients\n",
    "        dldu, dldv, dldw = self,bptt(x,y)\n",
    "        self.U -= learning_rate * dldu\n",
    "        self.V -= learning_rate * dldv\n",
    "        self.W -= learning_rate * dldw\n",
    "    \n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        o,s = self.forward_prop(x)\n",
    "        dldu = np.zeros(self.U.shape)\n",
    "        dldv = np.zeros(self.V.shape)\n",
    "        dldw = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dldv += np.outer(delta_o[t], s[t].T)\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1-(s[t] ** 2))\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                dldw += np.outer(delta_t, s[bptt_step-1])\n",
    "                dldu[:, x[bptt_step]] += delta_t\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "        return [dldu, dldv, dldw]\n",
    "    \n",
    "    def gradient_check(self, x, y, h = 0.001, error_threshold = 0.01):\n",
    "        bptt_gradients = model.bptt(x,y)\n",
    "        model_parameters = [\"U\", \"V\", \"W\"]\n",
    "        for pidx, pname in enumerate(model_parameters):\n",
    "            parameter = operator.attrgetter(pname)(self)\n",
    "            print(\"Performing gradient check for parameter {} with size {}\".format(pname, np.prod(parameter.shape)))\n",
    "            it = np.idter(parameter, flags = ['multi_index'], op_flags = ['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multiindex\n",
    "                original_value = parameter[ix]\n",
    "                parameter[ix] = original_value + h\n",
    "                gradplus = model.calculate_total_loss([x],[y])\n",
    "                parameter[ix] =original_value - h\n",
    "                gradminus = model.calculate_total_loss([x],[y])\n",
    "                estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "                parameter[ix] = original_value\n",
    "                backprop_gradient = bptt_gradients[pidx][ix]\n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "                if relative_error > error_threshold:\n",
    "                    print(\"Gradient Check ERROR: parameters {} ix = {}\".format(pname, ix))\n",
    "                    print(\"+h loss: \", gradplus)\n",
    "                    print(\"-h loss: \", gradminus)\n",
    "                    print(\"Estimated_gradient: \", estimated_gradient)\n",
    "                    print(\"Backprop gradient: \", backprop_gradient)\n",
    "                    print(\"Relative Error: \", relative_error)\n",
    "                    return\n",
    "                it.iternext()\n",
    "            print(\"Gradient check passed for parameter: \", pname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (50,8000) and (50,) not aligned: 8000 (dim 1) != 50 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c3baa316c872>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-266b622b574a>\u001b[0m in \u001b[0;36mforward_prop\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;31m# indexing U by x[t] is the same as multiplying U with One Hot vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0ms_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_in\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mouts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (50,8000) and (50,) not aligned: 8000 (dim 1) != 50 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNN(vocabulary_size)\n",
    "o, s = model.forward_prop(x_train[10])\n",
    "print(o.shape)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_train[10])\n",
    "print(predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([index_to_word[w] for w in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Expected Loss for random predictions: \", np.log(vocabulary_size))\n",
    "print(\"Actual Loss: \", model.calculate_total_loss(x_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNN(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sgd(model, X_train, y_train, learning_rate = 0.005, nepoch = 100, evaluate_loss_after = 5):\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        if (epoch%evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(x_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(\"{}: loss after num_exampls_seen = {} epoch = {} : {}\".format(timie,num_examples_seen, epoch, loss))\n",
    "            if (len(losses)>1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate *= 0.5\n",
    "                print(\"Setting Learning rate to: \", learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        for i in range(len(y_train)):\n",
    "            model.sgd_step(x_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "model = RNN(vocabulary_size)\n",
    "train_with_sgd(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model):\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    while (not new_sentence[-1]==word_to_index[sentence_end_token]):\n",
    "        next_word_probs, states = model.forward_prop(new_sentence)\n",
    "        samples_word = word_to_index[unknown_token]\n",
    "        \n",
    "        while samples_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[l:-1]]\n",
    "    return sentence_str\n",
    "\n",
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print(\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
